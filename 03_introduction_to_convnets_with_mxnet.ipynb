{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_introduction_to_convnets_with_mxnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningMXNet/blob/master/03_introduction_to_convnets_with_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx2LMiaPLoYO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Convolutional Neural Networks with MXNet\n",
        "\n",
        "We previously trained an artificial neural network on the MNIST dataset, now we'll introduce the convolutional neural networks (CNNs or Convnets for short). CNNs are part of the world of deep learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBaWG-3uKPh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5MHnLRfNJnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3xK19PbOnk9",
        "colab_type": "code",
        "outputId": "e0491de7-6aef-40e9-d7d5-5a0ff90e750b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "\n",
        "import statistics\n",
        "\n",
        "\n",
        "print(mx.__version__)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Yk4yHuO7Pr",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "As always we define the transformations to be applied to our dataset. In this case we use ToTensor to change the shape of our tensors from (H, W, C) to (C, H, W), and to normalize the date so that it's in the range [0, 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfYNedeoO-si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.data.vision.transforms.ToTensor.html\n",
        "# (HxWxC), [0, 255] -> (CxHxW), [0, 1)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "MNIST = gluon.data.vision.MNIST\n",
        "\n",
        "train_data = MNIST(train=True).transform_first(transform)\n",
        "valid_data = MNIST(train=False).transform_first(transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKKsMQYbQknB",
        "colab_type": "code",
        "outputId": "becf0d17-45af-425c-9600-122893c5e4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_loader = gluon.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "valid_loader = gluon.data.DataLoader(valid_data, shuffle=False, batch_size=64)\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "\n",
        "batch, labels = dataiter.__next__()\n",
        "\n",
        "print(batch.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1, 28, 28)\n",
            "(64,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0FR5If1mqbD",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9W4BzdsjZeN",
        "colab_type": "code",
        "outputId": "a9a5f376-e34b-4b58-d5df-21b29ee1c5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "convnet = nn.Sequential()\n",
        "\n",
        "convnet.add(\n",
        "    nn.Conv2D(channels=32, kernel_size=3, activation='relu'),\n",
        "    nn.MaxPool2D(pool_size=2),\n",
        "    nn.Conv2D(channels=64, kernel_size=3, activation='relu'),\n",
        "    nn.MaxPool2D(pool_size=2),\n",
        "    nn.Conv2D(channels=64, kernel_size=3, activation='relu'),\n",
        "    nn.Dense(64, activation='relu'),\n",
        "    nn.Dense(10)\n",
        ")\n",
        "\n",
        "convnet"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), Activation(relu))\n",
              "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
              "  (2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), Activation(relu))\n",
              "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
              "  (4): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), Activation(relu))\n",
              "  (5): Dense(None -> 64, Activation(relu))\n",
              "  (6): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAtEBG72rpY4",
        "colab_type": "code",
        "outputId": "cedcf249-a112-4e9a-fc3f-aa86ae6f7bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\n",
        "convnet.initialize(mx.init.Xavier(), ctx=ctx)\n",
        "convnet.summary(nd.zeros((1, 1, 28, 28), ctx=ctx))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "        Layer (type)                                Output Shape         Param #\n",
            "================================================================================\n",
            "               Input                              (1, 1, 28, 28)               0\n",
            "        Activation-1                     <Symbol conv3_relu_fwd>               0\n",
            "        Activation-2                             (1, 32, 26, 26)               0\n",
            "            Conv2D-3                             (1, 32, 26, 26)             320\n",
            "         MaxPool2D-4                             (1, 32, 13, 13)               0\n",
            "        Activation-5                     <Symbol conv4_relu_fwd>               0\n",
            "        Activation-6                             (1, 64, 11, 11)               0\n",
            "            Conv2D-7                             (1, 64, 11, 11)           18496\n",
            "         MaxPool2D-8                               (1, 64, 5, 5)               0\n",
            "        Activation-9                     <Symbol conv5_relu_fwd>               0\n",
            "       Activation-10                               (1, 64, 3, 3)               0\n",
            "           Conv2D-11                               (1, 64, 3, 3)           36928\n",
            "       Activation-12                    <Symbol dense2_relu_fwd>               0\n",
            "       Activation-13                                     (1, 64)               0\n",
            "            Dense-14                                     (1, 64)           36928\n",
            "            Dense-15                                     (1, 10)             650\n",
            "================================================================================\n",
            "Parameters in forward computation graph, duplicate included\n",
            "   Total params: 93322\n",
            "   Trainable params: 93322\n",
            "   Non-trainable params: 0\n",
            "Shared params in forward computation graph: 0\n",
            "Unique parameters in model: 93322\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzRPKI9ZvzkZ",
        "colab_type": "text"
      },
      "source": [
        "### Trainer: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiMQF9BkskbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(\n",
        "    params=convnet.collect_params(),\n",
        "    optimizer='sgd',\n",
        "    optimizer_params={'learning_rate': 0.04},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a86Mfx5q6bT",
        "colab_type": "text"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onvNDqIPi94y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_function, optimizer):\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            output = model(batch)\n",
        "            loss = loss_function(output, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        train_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    mean_loss = statistics.mean(train_batch_losses)\n",
        "    \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLEj_u94rD0n",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrBt3KaNk3Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_function, optimizer):\n",
        "    \n",
        "    validation_batch_losses = []\n",
        "    \n",
        "    for batch, labels in valid_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        output = model(batch)\n",
        "        \n",
        "        loss = loss_function(output, labels)\n",
        "        \n",
        "        validation_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "        mean_loss = statistics.mean(validation_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4XwJTsprHQw",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7T5Ui4TpvLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    \n",
        "    metric = mx.metric.Accuracy()\n",
        "    \n",
        "    for batch, labels in loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        class_probabilities = nd.softmax(model(batch), axis=1)\n",
        "        \n",
        "        predictions = nd.argmax(class_probabilities, axis=1)\n",
        "        \n",
        "        metric.update(labels, predictions)\n",
        "        \n",
        "    _, accuracy_metric = metric.get()\n",
        "    \n",
        "    return accuracy_metric * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtY3jhL9WEcQ",
        "colab_type": "text"
      },
      "source": [
        "**Training stats function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOEhrpsKWDKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_stats(train_loss, train_accuracy, val_loss, val_accuracy):\n",
        "    print(('training loss: {:.3f} '\n",
        "           'training accuracy: {:.2f}% || '\n",
        "           'val. loss: {:.3f} '\n",
        "           'val. accuracy: {:.2f}%').format(train_loss, train_accuracy,\n",
        "                                            val_loss, val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs3mCDQZFseL",
        "colab_type": "text"
      },
      "source": [
        "### Training the Convolutional Neural Network\n",
        "\n",
        "Since we're automatically calculating the gradient through **autograd** module, we use the same code to train our neural network in the previous notebook, to train our brand new convnet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ji8CPBuF2g",
        "colab_type": "code",
        "outputId": "1575c280-77ca-4205-b95c-2796bff44f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(1, 1 + EPOCHS):\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch, EPOCHS))\n",
        "    \n",
        "    train_loss = train(convnet, loss_function, trainer)\n",
        "    train_accuracy = accuracy(convnet, train_loader)\n",
        "    \n",
        "    valid_loss = validate(convnet, loss_function, trainer)\n",
        "    valid_accuracy = accuracy(convnet, valid_loader)\n",
        "    \n",
        "    training_stats(train_loss, train_accuracy, valid_loss, valid_accuracy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "training loss: 27.398 training accuracy: 96.18% || val. loss: 6.948 val. accuracy: 96.59%\n",
            "Epoch 2/15\n",
            "training loss: 6.303 training accuracy: 97.79% || val. loss: 4.339 val. accuracy: 97.79%\n",
            "Epoch 3/15\n",
            "training loss: 4.470 training accuracy: 98.44% || val. loss: 3.178 val. accuracy: 98.35%\n",
            "Epoch 4/15\n",
            "training loss: 3.531 training accuracy: 98.35% || val. loss: 3.082 val. accuracy: 98.34%\n",
            "Epoch 5/15\n",
            "training loss: 2.949 training accuracy: 99.05% || val. loss: 2.234 val. accuracy: 98.83%\n",
            "Epoch 6/15\n",
            "training loss: 2.515 training accuracy: 99.19% || val. loss: 2.102 val. accuracy: 98.90%\n",
            "Epoch 7/15\n",
            "training loss: 2.208 training accuracy: 99.13% || val. loss: 2.141 val. accuracy: 98.93%\n",
            "Epoch 8/15\n",
            "training loss: 2.003 training accuracy: 99.17% || val. loss: 2.043 val. accuracy: 98.94%\n",
            "Epoch 9/15\n",
            "training loss: 1.766 training accuracy: 99.33% || val. loss: 1.993 val. accuracy: 98.86%\n",
            "Epoch 10/15\n",
            "training loss: 1.544 training accuracy: 98.67% || val. loss: 3.045 val. accuracy: 98.50%\n",
            "Epoch 11/15\n",
            "training loss: 1.384 training accuracy: 99.24% || val. loss: 1.963 val. accuracy: 99.01%\n",
            "Epoch 12/15\n",
            "training loss: 1.252 training accuracy: 98.32% || val. loss: 3.961 val. accuracy: 98.05%\n",
            "Epoch 13/15\n",
            "training loss: 1.158 training accuracy: 99.63% || val. loss: 1.776 val. accuracy: 99.06%\n",
            "Epoch 14/15\n",
            "training loss: 1.059 training accuracy: 99.53% || val. loss: 2.174 val. accuracy: 99.00%\n",
            "Epoch 15/15\n",
            "training loss: 1.019 training accuracy: 99.56% || val. loss: 2.135 val. accuracy: 98.97%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}