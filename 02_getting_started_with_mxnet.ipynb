{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_getting_started_with_mxnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningMXNet/blob/master/02_getting_started_with_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At65R3wNpAqc",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with MXNet: Training a NN on MNIST\n",
        "\n",
        "In this notebook, we train an artificial neural network on the MNIST dataset. We'll build a very simple neural network of 3 layers (input, hidden and output), and use dropout for regularization.\n",
        "\n",
        "As we saw in the previous notebook, Mxnet is not installed by default in Colab. So first, we need to find out the CUDA version Colab is using and then install the right Mxnet package for the CUDA version, as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZvP86okM95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCouxzmhQJJI",
        "colab_type": "text"
      },
      "source": [
        "Colab is using CUDA 10.0 so we need to install mxnet-cu100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkJnMtQlYL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiM6UmcQV8c",
        "colab_type": "text"
      },
      "source": [
        "Now we'll import a couple of standard modules:\n",
        "\n",
        "- **mxnet** is the framework that we import as **mx**\n",
        "- **nd** is short for NDarray and is MXNet's primary tool for working with tensors\n",
        "- **gluon** includes several modules that we'll be using for training our network, such as **data** for downloading the dataset and loading the data into tensors, and **loss** for calculating the loss on each iteration.\n",
        "- **autograd** is the tool we use to automatically calculate the network's gradients w.r.t. the parameters\n",
        "- **nn** is a high-level API that will help us build our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH103yBYllN1",
        "colab_type": "code",
        "outputId": "d169904d-e03f-416e-bbcb-23a79a7f5647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "\n",
        "import statistics\n",
        "\n",
        "print(mx.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS_xGX7ZFev8",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "We are going to work with the MNIST dataset. Basically it contains images of handwritten digits in grayscale, and its corresponding labels (one, two, three, etc).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ps2Ca92Noe",
        "colab_type": "code",
        "outputId": "5b98f082-bc61-45b3-ab29-88c8a8b600e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "MNIST = gluon.data.vision.MNIST\n",
        "\n",
        "train_data = MNIST(train=True).transform_first(transform)\n",
        "valid_data = MNIST(train=False).transform_first(transform)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXnnPoT4C3q",
        "colab_type": "code",
        "outputId": "f4b8cea4-175c-4594-d850-6cb3154c189b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_loader = gluon.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "valid_loader = gluon.data.DataLoader(valid_data, shuffle=False, batch_size=64)\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "\n",
        "batch, labels = dataiter.__next__()\n",
        "\n",
        "print(batch.shape)\n",
        "print(labels.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1, 28, 28)\n",
            "(64,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBMAoiPu2QvL",
        "colab_type": "text"
      },
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "We use the Sequential container, which provides an API similar to Keras. We put together 4 different layers:\n",
        "\n",
        "- **Flatten:** before feed forwarding the MNIST images we need to stretch them out. So this layer gets a 28x28 matrix and turn it into a 784-elements array/vector.\n",
        "- **Dense (hidden layer):** this is our first fully connected layer. Each of its neurons connects to all 784 input neurons, and each has a bias. Also each neuron in this layer has ReLU as the activation function.\n",
        "- **Dropuout:** this is the regularization method we'll use when training our network. Dropout works by, in each iteration, dropping some of the neurons in the previous layer.\n",
        "- **Dense (output layer):** the MNIST dataset has 10 classes, each for each one of the digits. So we'll have 10 neurons in this layer, representing each of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rk0WhwYmOaZ",
        "colab_type": "code",
        "outputId": "7e615b62-2660-4a95-a0f8-8ac504c8052f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "drop_prob = 0.2\n",
        "\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Flatten(),\n",
        "        nn.Dense(128, activation='relu'),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.Dense(10))\n",
        "\n",
        "net"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten\n",
              "  (1): Dense(None -> 128, Activation(relu))\n",
              "  (2): Dropout(p = 0.2, axes=())\n",
              "  (3): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBye8v9630BZ",
        "colab_type": "text"
      },
      "source": [
        "Before initializing our network we'll setup a GPU device. We can either train our model via a CPU or a GPU. GPUs are designed and optimized for processing tensors (or arrays in general), and we can borrow a GPU from Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGL5yOzKxOpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\n",
        "net.initialize(mx.init.Xavier(), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2QIVEgWQ_Wi",
        "colab_type": "text"
      },
      "source": [
        "Now we call the summary method an take a look at our neural network's architecture. As we see, our basic neural network has 101,700 parameters to train, including weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v_g_DdIL-Fd",
        "colab_type": "code",
        "outputId": "4da10005-5027-403f-b647-dda109498b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "net.summary(nd.zeros((1, 1, 28, 28), ctx=ctx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "        Layer (type)                                Output Shape         Param #\n",
            "================================================================================\n",
            "               Input                              (1, 1, 28, 28)               0\n",
            "           Flatten-1                                    (1, 784)               0\n",
            "        Activation-2                    <Symbol dense0_relu_fwd>               0\n",
            "        Activation-3                                    (1, 128)               0\n",
            "             Dense-4                                    (1, 128)          100480\n",
            "           Dropout-5                                    (1, 128)               0\n",
            "             Dense-6                                     (1, 10)            1290\n",
            "================================================================================\n",
            "Parameters in forward computation graph, duplicate included\n",
            "   Total params: 101770\n",
            "   Trainable params: 101770\n",
            "   Non-trainable params: 0\n",
            "Shared params in forward computation graph: 0\n",
            "Unique parameters in model: 101770\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bYILuT83lgN",
        "colab_type": "text"
      },
      "source": [
        "### Trainer: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOYmG5Wyxu7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(\n",
        "    params=net.collect_params(),\n",
        "    optimizer='sgd',\n",
        "    optimizer_params={'learning_rate': 0.04},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGYb87SVdWZB",
        "colab_type": "text"
      },
      "source": [
        "**Train function**\n",
        "\n",
        "The train function will train our artificial neural network by finding the parameters that minimize the loss function. Also we are keeping track of the losses as scalars, and at the end we calculate the mean loss. Here are the train function's steps:\n",
        "\n",
        "**(i)** get a batch of training examples and its labels, and send them to the GPU (CUDA), **(ii)** capture the code whose gradients will be calculated through autograd, **(iii)** forward propagate the batch through the NN and calculate the loss, **(iv)** backpropagate the loss through the NN and update the parameters (weights and biases).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKsZlitXM1IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_function, optimizer):\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            #these are the output layer's values before applying softmax\n",
        "            output = model(batch)\n",
        "            #the loss function applies softmax to the output\n",
        "            loss = loss_function(output, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        train_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    batch_loss = statistics.mean(train_batch_losses)\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IzqcMIwHPIj",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**\n",
        "\n",
        "Once we have trained our neural network we are ready to validate our model using our validation/test set. The validation function goes through the validation set and outputs the mean loss. At this point we are only working with the loss, we'll calculate the accuracy using a different function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SSGsgObpQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_function, optimizer):\n",
        "    \n",
        "    validation_batch_losses = []\n",
        "    \n",
        "    for batch, labels in valid_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        #these are the output layer's values before applying softmax\n",
        "        output = model(batch)\n",
        "        #the loss function applies softmax to the output\n",
        "        loss = loss_function(output, labels)\n",
        "        \n",
        "        validation_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "        mean_loss = statistics.mean(validation_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AWWtfo0HTLo",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**\n",
        "\n",
        "We need to know how well is doing our model at predicting the digits for each image. In the accuracy function we use the Accuracy metric that is included in mxnet.\n",
        "\n",
        "Since the loss function includes the Softmax activation, our neural network's outputs are raw numbers. So we use **nd.softmax** to get the NN's probabilities for each class/digit, and use **nd.argmax** to get the prediction for each training or validation example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAlb7wRX_yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    \n",
        "    metric = mx.metric.Accuracy()\n",
        "    \n",
        "    for batch, labels in loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        class_probabilities = nd.softmax(model(batch), axis=1)\n",
        "        \n",
        "        predictions = nd.argmax(class_probabilities, axis=1)\n",
        "        \n",
        "        metric.update(labels, predictions)\n",
        "        \n",
        "    _, accuracy_metric = metric.get()\n",
        "    \n",
        "    return accuracy_metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEC1HtEGp_0",
        "colab_type": "text"
      },
      "source": [
        "### Training the Neural Network\n",
        "\n",
        "Now it's time to train our NN and the first step is to define the loss function. We then define the number of epochs we'll use; in this case an epoch is a training cycle which means that we go through the whole training set and get the parameters at the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMQE_NOSVtxq",
        "colab_type": "code",
        "outputId": "4f341357-c914-40bf-fa99-a28807512960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(1, 1 + epochs):\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch, epochs))\n",
        "    \n",
        "    train_loss = train(net, loss_function, trainer)\n",
        "    train_accuracy = accuracy(net, train_loader)\n",
        "    \n",
        "    print('Training loss: {}'.format(train_loss))\n",
        "    print('Training accuracy: {}%'.format(train_accuracy * 100))\n",
        "    \n",
        "    valid_loss = validate(net, loss_function, trainer)\n",
        "    valid_accuracy = accuracy(net, valid_loader)\n",
        "    \n",
        "    print('Validation loss: {}'.format(valid_loss))\n",
        "    print('Validation accuracy: {}%'.format(valid_accuracy * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Training loss: 12.570566440187791\n",
            "Training accuracy: 95.66666666666667%\n",
            "Validation loss: 9.933800904803975\n",
            "Validation accuracy: 95.49%\n",
            "Epoch 2/10\n",
            "Training loss: 11.382317298892211\n",
            "Training accuracy: 96.06%\n",
            "Validation loss: 9.063731764152552\n",
            "Validation accuracy: 95.93%\n",
            "Epoch 3/10\n",
            "Training loss: 10.39353093638349\n",
            "Training accuracy: 96.42166666666667%\n",
            "Validation loss: 8.31701706776953\n",
            "Validation accuracy: 96.26%\n",
            "Epoch 4/10\n",
            "Training loss: 9.69043880917116\n",
            "Training accuracy: 96.71666666666667%\n",
            "Validation loss: 7.8181378755030355\n",
            "Validation accuracy: 96.48%\n",
            "Epoch 5/10\n",
            "Training loss: 9.099464911896028\n",
            "Training accuracy: 96.985%\n",
            "Validation loss: 7.436347416157176\n",
            "Validation accuracy: 96.59%\n",
            "Epoch 6/10\n",
            "Training loss: 8.548601573337116\n",
            "Training accuracy: 97.19166666666666%\n",
            "Validation loss: 6.99420712537067\n",
            "Validation accuracy: 96.8%\n",
            "Epoch 7/10\n",
            "Training loss: 8.11385058886461\n",
            "Training accuracy: 97.38666666666667%\n",
            "Validation loss: 6.658243758758163\n",
            "Validation accuracy: 96.97%\n",
            "Epoch 8/10\n",
            "Training loss: 7.708401306987063\n",
            "Training accuracy: 97.5%\n",
            "Validation loss: 6.529393875912117\n",
            "Validation accuracy: 97.00999999999999%\n",
            "Epoch 9/10\n",
            "Training loss: 7.363451302686988\n",
            "Training accuracy: 97.64166666666667%\n",
            "Validation loss: 6.234697233624519\n",
            "Validation accuracy: 97.24000000000001%\n",
            "Epoch 10/10\n",
            "Training loss: 7.0715586049978665\n",
            "Training accuracy: 97.76666666666667%\n",
            "Validation loss: 5.989811824812631\n",
            "Validation accuracy: 97.32%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}