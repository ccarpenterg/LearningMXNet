{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_getting_started_with_mxnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningMXNet/blob/master/02_getting_started_with_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At65R3wNpAqc",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with MXNet: Training a NN on MNIST\n",
        "\n",
        "In this notebook, we train an artificial neural network on the MNIST dataset. We'll build a very simple neural network of 3 layers (input, hidden and output), and use dropout for regularization.\n",
        "\n",
        "As we saw in the previous notebook, Mxnet is not installed by default in Colab. So first, we need to find out the CUDA version Colab is using and then install the right Mxnet package for the CUDA version, as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZvP86okM95",
        "colab_type": "code",
        "outputId": "41b5ed45-59a0-41fc-aeaf-fbb86476ce3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCouxzmhQJJI",
        "colab_type": "text"
      },
      "source": [
        "Colab is using CUDA 10.0 so we need to install mxnet-cu100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkJnMtQlYL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiM6UmcQV8c",
        "colab_type": "text"
      },
      "source": [
        "Now we'll import a couple of standard modules:\n",
        "\n",
        "- **mxnet** is the framework that we import as **mx**\n",
        "- **nd** is short for NDarray and is MXNet's primary tool for working with tensors\n",
        "- **gluon** includes several modules that we'll be using for training our network, such as **data** for downloading the dataset and loading the data into tensors, and **loss** for calculating the loss on each iteration.\n",
        "- **autograd** is the tool we use to automatically calculate the network's gradients w.r.t. the parameters\n",
        "- **nn** is a high-level API that will help us build our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH103yBYllN1",
        "colab_type": "code",
        "outputId": "99d9c696-5422-4923-e0fc-ee0d3cb1d4a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "import statistics\n",
        "\n",
        "print(mx.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS_xGX7ZFev8",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "We are going to work with the MNIST dataset. Basically it contains images of handwritten digits in grayscale, and its corresponding labels (one, two, three, etc).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ps2Ca92Noe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# MXNet's default data convention is NCHW whereas\n",
        "# the MNIST Tensor's dimensions are NHWC\n",
        "\n",
        "def data_convention_normalization(data):\n",
        "    \"\"\"HWC -> CHW; Move the channel axis (2) to the first axis (0)\"\"\"\n",
        "    return nd.moveaxis(data, 2, 0).astype('float32') / 255\n",
        "\n",
        "\n",
        "train_data = gluon.data.vision.MNIST(train=True).transform_first(data_convention_normalization)\n",
        "val_data = gluon.data.vision.MNIST(train=False).transform_first(data_convention_normalization)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXnnPoT4C3q",
        "colab_type": "code",
        "outputId": "223d837e-ccce-4722-f334-3a8495ce140b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_loader = gluon.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "val_loader = gluon.data.DataLoader(val_data, shuffle=False, batch_size=64)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    pass\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1, 28, 28)\n",
            "(32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBMAoiPu2QvL",
        "colab_type": "text"
      },
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "We use the Sequential container, which provides an API similar to Keras. We put together 4 different layers:\n",
        "\n",
        "- Flatten: before feed forwarding the MNIST images we need to stretch them out. So this layer gets a 28x28 matrix and turn it into a 784-elements array/vector.\n",
        "- Dense (hidden layer): this is our first fully connected layer. Each of its neurons connects to all 784 input neurons, and each has a bias.\n",
        "- Dropuout: \n",
        "- Dense (output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rk0WhwYmOaZ",
        "colab_type": "code",
        "outputId": "f68bb819-bccc-4f5c-b5b6-1ffa15454248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "drop_prob = 0.2\n",
        "\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Flatten(),\n",
        "        nn.Dense(128, activation='relu'),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.Dense(10))\n",
        "\n",
        "net"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten\n",
              "  (1): Dense(None -> 128, Activation(relu))\n",
              "  (2): Dropout(p = 0.2, axes=())\n",
              "  (3): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBye8v9630BZ",
        "colab_type": "text"
      },
      "source": [
        "Before initializing our network we'll setup a GPU device. We can either train our model via a CPU or a GPU. GPUs are designed and optimized for processing tensors (or arrays in general), and we can borrow a GPU from Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGL5yOzKxOpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\n",
        "net.initialize(mx.init.Xavier(), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bYILuT83lgN",
        "colab_type": "text"
      },
      "source": [
        "### Trainer: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOYmG5Wyxu7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(\n",
        "    params=net.collect_params(),\n",
        "    optimizer='sgd',\n",
        "    optimizer_params={'learning_rate': 0.04},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5DOYC3KyHym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_accuracy = mx.metric.Accuracy()\n",
        "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtFddDIgyB8",
        "colab_type": "code",
        "outputId": "9330eb2a-e347-4296-e911-223d8cafaa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    batch_train_loss = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        \n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            predictions = net(batch)\n",
        "            loss = loss_function(predictions, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        training_accuracy.update(labels, predictions)\n",
        "        \n",
        "        trainer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        batch_train_loss.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    batch_loss = statistics.mean(batch_train_loss)\n",
        "    \n",
        "    name, train_accuracy = training_accuracy.get()\n",
        "    \n",
        "    \n",
        "    print('Loss on epoch {}: {}'.format(epoch + 1, batch_loss))\n",
        "    print('Training accuracy on epoch {}: {}'.format(epoch + 1, train_accuracy))\n",
        "    training_accuracy.reset()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss on epoch 1: 7.28733363664989\n",
            "Training accuracy on epoch 1: 0.96725\n",
            "Loss on epoch 2: 7.00827365554472\n",
            "Training accuracy on epoch 2: 0.96865\n",
            "Loss on epoch 3: 6.838372502372717\n",
            "Training accuracy on epoch 3: 0.9687666666666667\n",
            "Loss on epoch 4: 6.499931328395791\n",
            "Training accuracy on epoch 4: 0.97085\n",
            "Loss on epoch 5: 6.231258062538561\n",
            "Training accuracy on epoch 5: 0.9714833333333334\n",
            "Loss on epoch 6: 6.0827142206717655\n",
            "Training accuracy on epoch 6: 0.97295\n",
            "Loss on epoch 7: 5.839221737659308\n",
            "Training accuracy on epoch 7: 0.9736666666666667\n",
            "Loss on epoch 8: 5.636277662093706\n",
            "Training accuracy on epoch 8: 0.9739333333333333\n",
            "Loss on epoch 9: 5.594816475217022\n",
            "Training accuracy on epoch 9: 0.9735833333333334\n",
            "Loss on epoch 10: 5.429911271944992\n",
            "Training accuracy on epoch 10: 0.9757333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9M_K-5PMKnY",
        "colab_type": "code",
        "outputId": "2ad4032d-4f2d-4afc-db26-7e8d2e026340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metric = mx.metric.Accuracy()\n",
        "for batch, labels in val_loader:\n",
        "    batch = batch.as_in_context(ctx)\n",
        "    labels = labels.as_in_context(ctx)\n",
        "    metric.update(labels, net(batch))\n",
        "    \n",
        "print('Validation: {} = {}'.format(*metric.get()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation: accuracy = 0.9777\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}