{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_getting_started_with_mxnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningMXNet/blob/master/02_getting_started_with_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At65R3wNpAqc",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with MXNet: Training a NN on MNIST\n",
        "\n",
        "In this notebook, we train an artificial neural network on the MNIST dataset. We'll build a very simple neural network of 3 layers (input, hidden and output), and use dropout for regularization.\n",
        "\n",
        "As we saw in the previous notebook, Mxnet is not installed by default in Colab. So first, we need to find out the CUDA version Colab is using and then install the right Mxnet package for the CUDA version, as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZvP86okM95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCouxzmhQJJI",
        "colab_type": "text"
      },
      "source": [
        "Colab is using CUDA 10.0 so we need to install mxnet-cu100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkJnMtQlYL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiM6UmcQV8c",
        "colab_type": "text"
      },
      "source": [
        "Now we'll import a couple of standard modules:\n",
        "\n",
        "- **mxnet** is the framework that we import as **mx**\n",
        "- **nd** is short for NDarray and is MXNet's primary tool for working with tensors\n",
        "- **gluon** includes several modules that we'll be using for training our network, such as **data** for downloading the dataset and loading the data into tensors, and **loss** for calculating the loss on each iteration.\n",
        "- **autograd** is the tool we use to automatically calculate the network's gradients w.r.t. the parameters\n",
        "- **nn** is a high-level API that will help us build our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH103yBYllN1",
        "colab_type": "code",
        "outputId": "064f6a5b-ea8e-43d4-8855-96ec3a9777de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "\n",
        "import statistics\n",
        "\n",
        "print(mx.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS_xGX7ZFev8",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "We are going to work with the MNIST dataset. Basically it contains images of handwritten digits in grayscale, and its corresponding labels (one, two, three, etc).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ps2Ca92Noe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "MNIST = gluon.data.vision.MNIST\n",
        "\n",
        "train_data = MNIST(train=True).transform_first(transform)\n",
        "valid_data = MNIST(train=False).transform_first(transform)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56COhqWc5MS",
        "colab_type": "text"
      },
      "source": [
        "We'll train and validate our neural network using batches, and for that Mxnet provides a DataLoader module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXnnPoT4C3q",
        "colab_type": "code",
        "outputId": "e1194181-6a93-4756-99c9-40bf57e6ca55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_loader = gluon.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "valid_loader = gluon.data.DataLoader(valid_data, shuffle=False, batch_size=64)\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "\n",
        "batch, labels = next(dataiter)\n",
        "\n",
        "print(batch.shape)\n",
        "print(labels.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1, 28, 28)\n",
            "(64,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqvRSNNqdOIg",
        "colab_type": "text"
      },
      "source": [
        "We just defined two dataloader, one for the training data and one for the validation data. When examine the training loader we get 64 MNIST images (28x28 pixles, 1 color channel) and its corresponding 64 labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBMAoiPu2QvL",
        "colab_type": "text"
      },
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "To build our network, we'll use the Sequential container which provides an API similar to Keras. We put together 4 different layers:\n",
        "\n",
        "- **Flatten:** before feed forwarding the MNIST images we need to stretch them out. So this layer gets a 28x28 matrix and turn it into a 784-elements array/vector, so it can be processed by the next layer.\n",
        "- **Dense (hidden layer):** this is our first fully connected layer. Each of its neurons connects to all 784 input neurons, and each has a bias. Also each neuron in this layer has ReLU as the activation function.\n",
        "- **Dropuout:** this is the regularization method we'll use when training our network. Dropout works by, in each iteration, dropping some of the neurons in the previous layer.\n",
        "- **Dense (output layer):** the MNIST dataset has 10 classes, each for each one of the digits. So we'll have 10 neurons in this layer, representing each of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rk0WhwYmOaZ",
        "colab_type": "code",
        "outputId": "f1b45d0b-2f2b-49b4-aa1b-48a083f14015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# dropout rate of 0.2 means that a neuron has a 20% probability of being dropped\n",
        "drop_prob = 0.2\n",
        "\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Flatten(),\n",
        "        nn.Dense(128, activation='relu'),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.Dense(10))\n",
        "\n",
        "net"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten\n",
              "  (1): Dense(None -> 128, Activation(relu))\n",
              "  (2): Dropout(p = 0.2, axes=())\n",
              "  (3): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBye8v9630BZ",
        "colab_type": "text"
      },
      "source": [
        "Before initializing our network we'll setup a GPU device. We can either train our model via a CPU or a GPU. GPUs are designed and optimized for processing tensors (or arrays in general), and we can borrow a GPU from Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGL5yOzKxOpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\n",
        "net.initialize(mx.init.Xavier(), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2QIVEgWQ_Wi",
        "colab_type": "text"
      },
      "source": [
        "Now we call the summary method an take a look at our neural network's architecture. As we see, our basic neural network has 101,700 parameters to train, including weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v_g_DdIL-Fd",
        "colab_type": "code",
        "outputId": "2e98b56a-d4ca-48b5-94b1-585577bed194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "net.summary(nd.zeros((64, 1, 28, 28), ctx=ctx))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "        Layer (type)                                Output Shape         Param #\n",
            "================================================================================\n",
            "               Input                             (64, 1, 28, 28)               0\n",
            "           Flatten-1                                   (64, 784)               0\n",
            "        Activation-2                    <Symbol dense0_relu_fwd>               0\n",
            "        Activation-3                                   (64, 128)               0\n",
            "             Dense-4                                   (64, 128)          100480\n",
            "           Dropout-5                                   (64, 128)               0\n",
            "             Dense-6                                    (64, 10)            1290\n",
            "================================================================================\n",
            "Parameters in forward computation graph, duplicate included\n",
            "   Total params: 101770\n",
            "   Trainable params: 101770\n",
            "   Non-trainable params: 0\n",
            "Shared params in forward computation graph: 0\n",
            "Unique parameters in model: 101770\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bYILuT83lgN",
        "colab_type": "text"
      },
      "source": [
        "### Trainer: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOYmG5Wyxu7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(\n",
        "    params=net.collect_params(),\n",
        "    optimizer='sgd',\n",
        "    optimizer_params={'learning_rate': 0.04},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGYb87SVdWZB",
        "colab_type": "text"
      },
      "source": [
        "**Train function**\n",
        "\n",
        "The train function will train our artificial neural network by finding the parameters that minimize the loss function. Also we are keeping track of the losses as scalars, and at the end we calculate the mean loss. Here are the train function's steps:\n",
        "\n",
        "**(i)** get a batch of training examples and its labels, and send them to the GPU (CUDA), **(ii)** capture the code whose gradients will be calculated through autograd, **(iii)** forward propagate the batch through the NN and calculate the loss, **(iv)** backpropagate the loss through the NN and update the parameters (weights and biases).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKsZlitXM1IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_function, optimizer):\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            #these are the output layer's values before applying softmax\n",
        "            output = model(batch)\n",
        "            #the loss function applies softmax to the output\n",
        "            loss = loss_function(output, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        train_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    batch_loss = statistics.mean(train_batch_losses)\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IzqcMIwHPIj",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**\n",
        "\n",
        "Once we have trained our neural network we are ready to validate our model using our validation/test set. The validation function goes through the validation set and outputs the mean loss. At this point we are only working with the loss, we'll calculate the accuracy using a different function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SSGsgObpQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_function, optimizer):\n",
        "    \n",
        "    validation_batch_losses = []\n",
        "    \n",
        "    for batch, labels in valid_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        #these are the output layer's values before applying softmax\n",
        "        output = model(batch)\n",
        "        #the loss function applies softmax to the output\n",
        "        loss = loss_function(output, labels)\n",
        "        \n",
        "        validation_batch_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "        mean_loss = statistics.mean(validation_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AWWtfo0HTLo",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**\n",
        "\n",
        "We need to know how well is doing our model at predicting the digits for each image. In the accuracy function we use the Accuracy metric that is included in mxnet.\n",
        "\n",
        "Since the loss function includes the Softmax activation, our neural network's outputs are raw numbers. So we use **nd.softmax** to get the NN's probabilities for each class/digit, and use **nd.argmax** to get the prediction for each training or validation example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAlb7wRX_yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    \n",
        "    metric = mx.metric.Accuracy()\n",
        "    \n",
        "    for batch, labels in loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        class_probabilities = nd.softmax(model(batch), axis=1)\n",
        "        \n",
        "        predictions = nd.argmax(class_probabilities, axis=1)\n",
        "        \n",
        "        metric.update(labels, predictions)\n",
        "        \n",
        "    _, accuracy_metric = metric.get()\n",
        "    \n",
        "    return accuracy_metric * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFGpPgYuSfZp",
        "colab_type": "text"
      },
      "source": [
        "**Training statistics function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGZXiiKAOsCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_stats(train_loss, train_accuracy, val_loss, val_accuracy):\n",
        "    print(('training loss: {:.3f} '\n",
        "           'training accuracy: {:.2f}% || '\n",
        "           'val. loss: {:.3f} '\n",
        "           'val. accuracy: {:.2f}%').format(train_loss, train_accuracy,\n",
        "                                            val_loss, val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEC1HtEGp_0",
        "colab_type": "text"
      },
      "source": [
        "### Training the Neural Network\n",
        "\n",
        "Now it's time to train our NN and the first step is to define the loss function. We then define the number of epochs we'll use; in this case an epoch is a training cycle which means that we go through the whole training set and get the parameters at the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMQE_NOSVtxq",
        "colab_type": "code",
        "outputId": "f532880e-0e91-415a-a520-844b3d729ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, 1 + EPOCHS):\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch, EPOCHS))\n",
        "    \n",
        "    train_loss = train(net, loss_function, trainer)\n",
        "    train_accuracy = accuracy(net, train_loader)\n",
        "    \n",
        "    valid_loss = validate(net, loss_function, trainer)\n",
        "    valid_accuracy = accuracy(net, valid_loader)\n",
        "\n",
        "    training_stats(train_loss, train_accuracy, valid_loss, valid_accuracy)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "training loss: 7.478 training accuracy: 97.56% || val. loss: 6.396 val. accuracy: 97.07%\n",
            "Epoch 2/20\n",
            "training loss: 7.187 training accuracy: 97.68% || val. loss: 6.205 val. accuracy: 97.16%\n",
            "Epoch 3/20\n",
            "training loss: 6.917 training accuracy: 97.78% || val. loss: 6.070 val. accuracy: 97.20%\n",
            "Epoch 4/20\n",
            "training loss: 6.602 training accuracy: 97.94% || val. loss: 5.839 val. accuracy: 97.32%\n",
            "Epoch 5/20\n",
            "training loss: 6.419 training accuracy: 97.96% || val. loss: 5.761 val. accuracy: 97.34%\n",
            "Epoch 6/20\n",
            "training loss: 6.199 training accuracy: 98.09% || val. loss: 5.502 val. accuracy: 97.46%\n",
            "Epoch 7/20\n",
            "training loss: 6.033 training accuracy: 98.17% || val. loss: 5.532 val. accuracy: 97.56%\n",
            "Epoch 8/20\n",
            "training loss: 5.811 training accuracy: 98.19% || val. loss: 5.339 val. accuracy: 97.67%\n",
            "Epoch 9/20\n",
            "training loss: 5.586 training accuracy: 98.32% || val. loss: 5.232 val. accuracy: 97.62%\n",
            "Epoch 10/20\n",
            "training loss: 5.453 training accuracy: 98.39% || val. loss: 5.189 val. accuracy: 97.70%\n",
            "Epoch 11/20\n",
            "training loss: 5.317 training accuracy: 98.39% || val. loss: 5.033 val. accuracy: 97.67%\n",
            "Epoch 12/20\n",
            "training loss: 5.116 training accuracy: 98.47% || val. loss: 4.980 val. accuracy: 97.75%\n",
            "Epoch 13/20\n",
            "training loss: 5.103 training accuracy: 98.55% || val. loss: 4.914 val. accuracy: 97.87%\n",
            "Epoch 14/20\n",
            "training loss: 4.889 training accuracy: 98.59% || val. loss: 4.859 val. accuracy: 97.83%\n",
            "Epoch 15/20\n",
            "training loss: 4.826 training accuracy: 98.66% || val. loss: 4.789 val. accuracy: 97.85%\n",
            "Epoch 16/20\n",
            "training loss: 4.725 training accuracy: 98.67% || val. loss: 4.756 val. accuracy: 97.81%\n",
            "Epoch 17/20\n",
            "training loss: 4.564 training accuracy: 98.76% || val. loss: 4.680 val. accuracy: 97.91%\n",
            "Epoch 18/20\n",
            "training loss: 4.537 training accuracy: 98.81% || val. loss: 4.603 val. accuracy: 97.87%\n",
            "Epoch 19/20\n",
            "training loss: 4.463 training accuracy: 98.83% || val. loss: 4.710 val. accuracy: 97.94%\n",
            "Epoch 20/20\n",
            "training loss: 4.340 training accuracy: 98.84% || val. loss: 4.608 val. accuracy: 97.98%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}