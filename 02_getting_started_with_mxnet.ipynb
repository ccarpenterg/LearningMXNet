{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_getting_started_with_mxnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningMXNet/blob/master/02_getting_started_with_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At65R3wNpAqc",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with MXNet: Training a NN on MNIST\n",
        "\n",
        "In this notebook, we train an artificial neural network on the MNIST dataset. We'll build a very simple neural network of 3 layers (input, hidden and output), and use dropout for regularization.\n",
        "\n",
        "As we saw in the previous notebook, Mxnet is not installed by default in Colab. So first, we need to find out the CUDA version Colab is using and then install the right Mxnet package for the CUDA version, as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZvP86okM95",
        "colab_type": "code",
        "outputId": "80513a38-2a90-44e9-edd1-66041d21eafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCouxzmhQJJI",
        "colab_type": "text"
      },
      "source": [
        "Colab is using CUDA 10.0 so we need to install mxnet-cu100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkJnMtQlYL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiM6UmcQV8c",
        "colab_type": "text"
      },
      "source": [
        "Now we'll import a couple of standard modules:\n",
        "\n",
        "- **mxnet** is the framework that we import as **mx**\n",
        "- **nd** is short for NDarray and is MXNet's primary tool for working with tensors\n",
        "- **gluon** includes several modules that we'll be using for training our network, such as **data** for downloading the dataset and loading the data into tensors, and **loss** for calculating the loss on each iteration.\n",
        "- **autograd** is the tool we use to automatically calculate the network's gradients w.r.t. the parameters\n",
        "- **nn** is a high-level API that will help us build our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH103yBYllN1",
        "colab_type": "code",
        "outputId": "2f4acc82-d1b9-4c3b-b3c5-2fda00d4af82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "import statistics\n",
        "\n",
        "print(mx.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS_xGX7ZFev8",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "We are going to work with the MNIST dataset. Basically it contains images of handwritten digits in grayscale, and its corresponding labels (one, two, three, etc).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ps2Ca92Noe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# MXNet's default data convention is NCHW whereas\n",
        "# the MNIST Tensor's dimensions are NHWC\n",
        "\n",
        "def data_convention_normalization(data):\n",
        "    \"\"\"HWC -> CHW; Move the channel axis (2) to the first axis (0)\"\"\"\n",
        "    return nd.moveaxis(data, 2, 0).astype('float32') / 255\n",
        "\n",
        "\n",
        "train_data = gluon.data.vision.MNIST(train=True).transform_first(data_convention_normalization)\n",
        "val_data = gluon.data.vision.MNIST(train=False).transform_first(data_convention_normalization)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXnnPoT4C3q",
        "colab_type": "code",
        "outputId": "540eaf30-bcb3-4319-8378-cd07419b0f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_loader = gluon.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "val_loader = gluon.data.DataLoader(val_data, shuffle=False, batch_size=64)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    pass\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1, 28, 28)\n",
            "(32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBMAoiPu2QvL",
        "colab_type": "text"
      },
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "We use the Sequential container, which provides an API similar to Keras. We put together 4 different layers:\n",
        "\n",
        "- **Flatten:** before feed forwarding the MNIST images we need to stretch them out. So this layer gets a 28x28 matrix and turn it into a 784-elements array/vector.\n",
        "- **Dense (hidden layer):** this is our first fully connected layer. Each of its neurons connects to all 784 input neurons, and each has a bias. Also each neuron in this layer has ReLU as the activation function.\n",
        "- **Dropuout:** this is the regularization method we'll use when training our network. Dropout works by, in each iteration, dropping some of the neurons in the previous layer.\n",
        "- **Dense (output layer):** the MNIST dataset has 10 classes, each for each one of the digits. So we'll have 10 neurons in this layer, representing each of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rk0WhwYmOaZ",
        "colab_type": "code",
        "outputId": "c64d9773-56ff-41fe-ba5b-62564795f551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "drop_prob = 0.2\n",
        "\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Flatten(),\n",
        "        nn.Dense(128, activation='relu'),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.Dense(10))\n",
        "\n",
        "net"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten\n",
              "  (1): Dense(None -> 128, Activation(relu))\n",
              "  (2): Dropout(p = 0.2, axes=())\n",
              "  (3): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBye8v9630BZ",
        "colab_type": "text"
      },
      "source": [
        "Before initializing our network we'll setup a GPU device. We can either train our model via a CPU or a GPU. GPUs are designed and optimized for processing tensors (or arrays in general), and we can borrow a GPU from Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGL5yOzKxOpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\n",
        "net.initialize(mx.init.Xavier(), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2QIVEgWQ_Wi",
        "colab_type": "text"
      },
      "source": [
        "Now we call the summary method an take a look at our neural network's architecture. As we see, our basic neural network has 101,700 parameters to train, including weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v_g_DdIL-Fd",
        "colab_type": "code",
        "outputId": "7c3a9362-a7d1-4171-a3ef-a18d5da8e42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "net.summary(nd.zeros((1, 1, 28, 28), ctx=ctx))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "        Layer (type)                                Output Shape         Param #\n",
            "================================================================================\n",
            "               Input                              (1, 1, 28, 28)               0\n",
            "           Flatten-1                                    (1, 784)               0\n",
            "        Activation-2                    <Symbol dense0_relu_fwd>               0\n",
            "        Activation-3                                    (1, 128)               0\n",
            "             Dense-4                                    (1, 128)          100480\n",
            "           Dropout-5                                    (1, 128)               0\n",
            "             Dense-6                                     (1, 10)            1290\n",
            "================================================================================\n",
            "Parameters in forward computation graph, duplicate included\n",
            "   Total params: 101770\n",
            "   Trainable params: 101770\n",
            "   Non-trainable params: 0\n",
            "Shared params in forward computation graph: 0\n",
            "Unique parameters in model: 101770\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bYILuT83lgN",
        "colab_type": "text"
      },
      "source": [
        "### Trainer: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOYmG5Wyxu7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(\n",
        "    params=net.collect_params(),\n",
        "    optimizer='sgd',\n",
        "    optimizer_params={'learning_rate': 0.04},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGYb87SVdWZB",
        "colab_type": "text"
      },
      "source": [
        "Now we set up the training accuracy, and the loss function. In this case, we use cross entropy with softmax as our loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5DOYC3KyHym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_accuracy = mx.metric.Accuracy()\n",
        "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKsZlitXM1IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_function, optimizer):\n",
        "    \n",
        "    batch_train_loss = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            predictions = model(batch)\n",
        "            loss = loss_function(predictions, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        batch_train_loss.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    batch_loss = statistics.mean(batch_train_loss)\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SSGsgObpQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_function, optimizer):\n",
        "    \n",
        "    validation_batch_losses = []\n",
        "    \n",
        "    for batch, labels in val_loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        predictions = model(batch)\n",
        "        \n",
        "        loss = loss_function(predictions, labels)\n",
        "        \n",
        "        validation_bass_losses.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "        mean_loss = statistics.mean(validation_bass_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAlb7wRX_yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    \n",
        "    metric = mx.metric.Accuracy()\n",
        "    \n",
        "    for batch, labels in loader:\n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        output = nd.softmax(model(batch), axis=1)\n",
        "        \n",
        "        predictions = nd.argmax(output, axis=1)\n",
        "        \n",
        "        metric.update(labels, predictions)\n",
        "        \n",
        "    _, accuracy_metric = metric.get()\n",
        "    \n",
        "    return accuracy_metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTScEJJvdy41",
        "colab_type": "text"
      },
      "source": [
        "Now we will train our network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtFddDIgyB8",
        "colab_type": "code",
        "outputId": "9330eb2a-e347-4296-e911-223d8cafaa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    batch_train_loss = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        \n",
        "        batch = batch.as_in_context(ctx)\n",
        "        labels = labels.as_in_context(ctx)\n",
        "        \n",
        "        with autograd.record():\n",
        "            predictions = net(batch)\n",
        "            loss = loss_function(predictions, labels)\n",
        "            \n",
        "        loss.backward()\n",
        "        training_accuracy.update(labels, predictions)\n",
        "        \n",
        "        trainer.step(batch_size=batch.shape[0])\n",
        "        \n",
        "        batch_train_loss.append(float(nd.sum(loss).asscalar()))\n",
        "        \n",
        "    batch_loss = statistics.mean(batch_train_loss)\n",
        "    \n",
        "    name, train_accuracy = training_accuracy.get()\n",
        "    \n",
        "    \n",
        "    print('Loss on epoch {}: {}'.format(epoch + 1, batch_loss))\n",
        "    print('Training accuracy on epoch {}: {}'.format(epoch + 1, train_accuracy))\n",
        "    training_accuracy.reset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss on epoch 1: 7.28733363664989\n",
            "Training accuracy on epoch 1: 0.96725\n",
            "Loss on epoch 2: 7.00827365554472\n",
            "Training accuracy on epoch 2: 0.96865\n",
            "Loss on epoch 3: 6.838372502372717\n",
            "Training accuracy on epoch 3: 0.9687666666666667\n",
            "Loss on epoch 4: 6.499931328395791\n",
            "Training accuracy on epoch 4: 0.97085\n",
            "Loss on epoch 5: 6.231258062538561\n",
            "Training accuracy on epoch 5: 0.9714833333333334\n",
            "Loss on epoch 6: 6.0827142206717655\n",
            "Training accuracy on epoch 6: 0.97295\n",
            "Loss on epoch 7: 5.839221737659308\n",
            "Training accuracy on epoch 7: 0.9736666666666667\n",
            "Loss on epoch 8: 5.636277662093706\n",
            "Training accuracy on epoch 8: 0.9739333333333333\n",
            "Loss on epoch 9: 5.594816475217022\n",
            "Training accuracy on epoch 9: 0.9735833333333334\n",
            "Loss on epoch 10: 5.429911271944992\n",
            "Training accuracy on epoch 10: 0.9757333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9M_K-5PMKnY",
        "colab_type": "code",
        "outputId": "2ad4032d-4f2d-4afc-db26-7e8d2e026340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metric = mx.metric.Accuracy()\n",
        "for batch, labels in val_loader:\n",
        "    batch = batch.as_in_context(ctx)\n",
        "    labels = labels.as_in_context(ctx)\n",
        "    metric.update(labels, net(batch))\n",
        "    \n",
        "print('Validation: {} = {}'.format(*metric.get()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation: accuracy = 0.9777\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}